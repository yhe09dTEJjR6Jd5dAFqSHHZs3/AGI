import os
import sys
import time
import json
import glob
import logging
import traceback
import warnings
import subprocess
import importlib
from importlib import metadata
from datetime import datetime

# ==========================================
# 1. æ¶ˆé™¤ç‰¹å®šçš„è­¦å‘Šä¿¡æ¯
# ==========================================
warnings.filterwarnings("ignore", category=FutureWarning, message=".*pynvml.*")
warnings.filterwarnings("ignore", category=FutureWarning, message=".*GradScaler.*")
warnings.filterwarnings("ignore", category=UserWarning, message=".*use_reentrant.*")
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3" # å±è”½éå¿…è¦çš„ C++ åº•å±‚è­¦å‘Š

# é…ç½®è¯¦ç»†æ—¥å¿—è¾“å‡º
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

AUTO_FIX_DEPS = os.environ.get("AGI_AUTO_FIX_DEPS", "1") == "1"
MAX_REPAIR_RESTARTS = 2
REPAIR_RESTART_COUNT = int(os.environ.get("AGI_REPAIR_RESTART_COUNT", "0"))
FORCE_REPAIR_ON_START = os.environ.get("AGI_FORCE_REPAIR_ON_START", "0") == "1"
MIN_TRANSFORMERS = "4.45.0"
MIN_TOKENIZERS = "0.20.0"


def _pip_install(requirements, force_reinstall=False):
    cmd = [sys.executable, "-m", "pip", "install", "--upgrade"] + requirements
    if force_reinstall:
        cmd.extend(["--force-reinstall", "--no-cache-dir"])
    logging.warning(f"æ£€æµ‹åˆ°ä¾èµ–é—®é¢˜ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        logging.error("âŒ è‡ªåŠ¨ä¿®å¤ä¾èµ–å¤±è´¥ã€‚pip stdout/stderr å¦‚ä¸‹ï¼š")
        logging.error(result.stdout.strip())
        logging.error(result.stderr.strip())
        raise RuntimeError("è‡ªåŠ¨ä¿®å¤ä¾èµ–å¤±è´¥")
    logging.info("âœ… ä¾èµ–åº“è‡ªåŠ¨ä¿®å¤å®Œæˆã€‚")


def _restart_current_process(force_repair=False):
    if REPAIR_RESTART_COUNT >= MAX_REPAIR_RESTARTS:
        raise RuntimeError("ä¾èµ–ä¿®å¤é‡å¯æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥ Python ç¯å¢ƒã€‚")

    os.environ["AGI_REPAIR_RESTART_COUNT"] = str(REPAIR_RESTART_COUNT + 1)
    if force_repair:
        os.environ["AGI_FORCE_REPAIR_ON_START"] = "1"
    logging.warning("å‡†å¤‡é‡å¯å½“å‰ Python è¿›ç¨‹ä»¥åº”ç”¨ä¾èµ–ä¿®å¤...")
    os.execv(sys.executable, [sys.executable] + sys.argv)


def _repair_dependencies_before_import():
    if not FORCE_REPAIR_ON_START:
        return

    logging.info("æ£€æµ‹åˆ°é‡å¯ä¿®å¤æ ‡è®°ï¼Œå…ˆåœ¨å…¨æ–°è¿›ç¨‹ä¸­é‡è£…å…³é”®ä¾èµ–...")
    _pip_install([
        "pip",
        f"transformers>={MIN_TRANSFORMERS}",
        f"tokenizers>={MIN_TOKENIZERS}",
        "accelerate",
        "qwen-vl-utils",
        "bitsandbytes",
        "torchvision",
    ], force_reinstall=True)
    os.environ["AGI_FORCE_REPAIR_ON_START"] = "0"
    _restart_current_process(force_repair=False)


_repair_dependencies_before_import()

# ==========================================
# 2. å…¨å±€å¼‚å¸¸å¤„ç†ï¼Œç»å¯¹é¿å…é™é»˜å¤±è´¥
# ==========================================
def global_exception_handler(exc_type, exc_value, exc_traceback):
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    logging.error("âŒ ç¨‹åºå‘ç”Ÿæœªæ•è·çš„è‡´å‘½é”™è¯¯ï¼è¯¦ç»†ä¿¡æ¯å¦‚ä¸‹ï¼š")
    logging.error("".join(traceback.format_exception(exc_type, exc_value, exc_traceback)))
    
sys.excepthook = global_exception_handler

# ==========================================
# 3. ä¾èµ–åº“ä¸¥æ ¼è‡ªæ£€ä¸æ·±åº¦å¯¼å…¥
# ==========================================
logging.info("æ­£åœ¨æ‰§è¡Œä¾èµ–åº“æ·±åº¦æ£€æŸ¥...")
try:
    # åŸºç¡€è‡ªåŠ¨åŒ–åº“
    import pyautogui
    import pygetwindow as gw
    import keyboard
    from PIL import ImageGrab, Image
    import psutil

    # æ ¸å¿ƒ AI åº“æ·±åº¦æ£€æŸ¥ (å‰¥ç¦» HF å»¶è¿ŸåŠ è½½çš„æ¨¡ç³ŠæŠ¥é”™)
    import torch
    import torchvision
    import bitsandbytes # 4-bit é‡åŒ–å¿…é¡»
    import accelerate
    from packaging import version

    def _get_installed_version(pkg_name):
        try:
            return metadata.version(pkg_name)
        except metadata.PackageNotFoundError:
            return None

    def _purge_modules(prefixes):
        removed = []
        for module_name in list(sys.modules.keys()):
            if any(module_name == p or module_name.startswith(f"{p}.") for p in prefixes):
                removed.append(module_name)
                sys.modules.pop(module_name, None)
        if removed:
            logging.info(f"å·²æ¸…ç†æ¨¡å—ç¼“å­˜: {', '.join(sorted(removed)[:8])} ... å…± {len(removed)} ä¸ª")

    def _cleanup_tokenizers_shadowing():
        """æ¸…ç†å¸¸è§çš„ tokenizers/decoders å‘½åå†²çªä¸æŸåç¼“å­˜ã€‚"""
        import site

        candidates = []
        for site_dir in site.getsitepackages() + [site.getusersitepackages()]:
            candidates.extend(glob.glob(os.path.join(site_dir, "decoders*")))
            candidates.extend(glob.glob(os.path.join(site_dir, "tokenizers*")))

        for item in sorted(set(candidates)):
            base = os.path.basename(item).lower()
            # ç¬¬ä¸‰æ–¹ decoders åŒ…ä¼šæ±¡æŸ“ `from tokenizers import decoders`
            should_remove = (
                base == "decoders" or base.startswith("decoders-")
                or base.startswith("tokenizers-") or base == "tokenizers"
            )
            if not should_remove:
                continue

            try:
                if os.path.isdir(item):
                    import shutil
                    shutil.rmtree(item, ignore_errors=False)
                else:
                    os.remove(item)
                logging.warning(f"å·²æ¸…ç†å¯èƒ½å†²çª/æŸåçš„åŒ…ç¼“å­˜: {item}")
            except Exception as cleanup_error:
                logging.warning(f"æ¸…ç†å†²çªæ–‡ä»¶å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼Œåç»­ä¼šå¼ºåˆ¶é‡è£…ï¼‰: {item} -> {cleanup_error}")

    def _import_transformers_stack():
        import transformers as _transformers

        if version.parse(_transformers.__version__) < version.parse(MIN_TRANSFORMERS):
            raise ImportError(
                f"transformers ç‰ˆæœ¬è¿‡ä½ ({_transformers.__version__})ï¼Œéœ€è¦ >= {MIN_TRANSFORMERS}"
            )

        # æŸäº›ç¯å¢ƒä¼šå‡ºç° tokenizers/decoders ABI ä¸åŒ¹é…å¯¼è‡´ DecodeStream ä¸¢å¤±
        tokenizers_version = _get_installed_version("tokenizers")
        if tokenizers_version is None:
            raise ImportError("æœªæ£€æµ‹åˆ° tokenizers åŒ…ï¼Œè¯·å…ˆå®‰è£…ã€‚")
        if version.parse(tokenizers_version) < version.parse(MIN_TOKENIZERS):
            raise ImportError(
                f"tokenizers ç‰ˆæœ¬è¿‡ä½ ({tokenizers_version})ï¼Œéœ€è¦ >= {MIN_TOKENIZERS}"
            )

        try:
            from tokenizers import decoders as _tokenizer_decoders
            if not hasattr(_tokenizer_decoders, "DecodeStream"):
                raise ImportError("tokenizers.decoders ç¼ºå°‘ DecodeStreamï¼Œç–‘ä¼¼å®‰è£…æŸå")
            decoders_file = getattr(_tokenizer_decoders, "__file__", "")
            if "site-packages" in decoders_file and "tokenizers" not in decoders_file.lower():
                raise ImportError(f"tokenizers.decoders æŒ‡å‘äº†å¼‚å¸¸è·¯å¾„: {decoders_file}")
        except Exception as tokenizer_error:
            raise ImportError(
                f"tokenizers å¯¼å…¥å¼‚å¸¸ï¼ˆå¸¸è§äºåŒ…æŸå/ç‰ˆæœ¬å†²çªï¼‰: {tokenizer_error}"
            ) from tokenizer_error

        from transformers import Qwen2VLForConditionalGeneration as _Qwen2VLForConditionalGeneration
        from transformers import AutoProcessor as _AutoProcessor

        return _transformers, _Qwen2VLForConditionalGeneration, _AutoProcessor

    try:
        transformers, Qwen2VLForConditionalGeneration, AutoProcessor = _import_transformers_stack()
    except Exception as first_error:
        logging.error("é¦–æ¬¡å¯¼å…¥ transformers/Qwen2-VL å¤±è´¥ï¼Œè¯¦ç»†åŸå› ï¼š")
        logging.error(traceback.format_exc())
        if not AUTO_FIX_DEPS:
            raise first_error
        _purge_modules(["tokenizers", "transformers", "decoders"])
        _cleanup_tokenizers_shadowing()
        subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", "decoders"], capture_output=True, text=True)
        logging.warning("æ£€æµ‹åˆ°å¯èƒ½çš„ tokenizers ABI/æ–‡ä»¶é”é—®é¢˜ï¼Œå°†é‡å¯åˆ°å¹²å‡€è¿›ç¨‹åå†é‡è£…ä¾èµ–ã€‚")
        _restart_current_process(force_repair=True)

    from qwen_vl_utils import process_vision_info

    logging.info("âœ… æ‰€æœ‰ä¾èµ–åº“æ£€æŸ¥é€šè¿‡ï¼")

except ImportError as e:
    logging.error("âŒ ä¾èµ–åº“å¯¼å…¥å¤±è´¥ï¼")
    logging.error(f"ç›´æ¥åŸå› : {e}")
    logging.error("è¯·ç¡®ä¿æ‰§è¡Œäº†ä»¥ä¸‹å‘½ä»¤ï¼š")
    logging.error("pip install --upgrade transformers tokenizers accelerate qwen-vl-utils bitsandbytes torchvision")
    logging.error("å®Œæ•´æŠ¥é”™å †æ ˆå¦‚ä¸‹ï¼š")
    traceback.print_exc()
    sys.exit(1)

# PyAutoGUI å®‰å…¨è®¾ç½®
pyautogui.FAILSAFE = False

# ==========================================
# 4. åˆå§‹åŒ–å‚æ•°ä¸è·¯å¾„
# ==========================================
DESKTOP_PATH = os.path.join(os.environ['USERPROFILE'], 'Desktop')
AAA_DIR = os.path.join(DESKTOP_PATH, 'AAA')
MODEL_DIR = os.path.join(AAA_DIR, 'Model')
EXP_DIR = os.path.join(AAA_DIR, 'Experience Pool')
FIREFOX_PATH = r"E:\FirefoxPortable\FirefoxPortable.exe"
MAX_EXP_SIZE = 20 * 1024 * 1024 * 1024  # 20 GB

# ==========================================
# 5. åˆå§‹åŒ–é˜¶æ®µï¼šæ£€æŸ¥ä¸åˆ›å»ºæ–‡ä»¶å¤¹
# ==========================================
def init_environment():
    logging.info("å¼€å§‹åˆå§‹åŒ–ç¯å¢ƒ...")
    for directory in [AAA_DIR, MODEL_DIR, EXP_DIR]:
        if not os.path.exists(directory):
            os.makedirs(directory)
            logging.info(f"å·²è‡ªåŠ¨ç”Ÿæˆç¼ºå¤±çš„æ–‡ä»¶å¤¹: {directory}")
        else:
            logging.info(f"æ£€æŸ¥æ–‡ä»¶å¤¹å­˜åœ¨: {directory}")
            
# ==========================================
# 6. ç»éªŒæ± ç®¡ç† (20GB é™åˆ¶ï¼Œä¸¢å¼ƒæ—§æ•°æ®)
# ==========================================
def get_dir_size(path):
    total_size = 0
    for dirpath, _, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            if not os.path.islink(fp):
                total_size += os.path.getsize(fp)
    return total_size

def manage_experience_pool():
    size = get_dir_size(EXP_DIR)
    if size > MAX_EXP_SIZE:
        logging.info(f"âš ï¸ ç»éªŒæ± å¤§å° ({size / 1024**3:.2f}GB) è¶…è¿‡ 20GBï¼Œå¼€å§‹æ¸…ç†æœ€æ—§æ•°æ®...")
        files = []
        for f in os.listdir(EXP_DIR):
            fp = os.path.join(EXP_DIR, f)
            if os.path.isfile(fp):
                files.append((fp, os.path.getmtime(fp), os.path.getsize(fp)))
        
        # æŒ‰ä¿®æ”¹æ—¶é—´ä»æ—§åˆ°æ–°æ’åº
        files.sort(key=lambda x: x[1])
        
        for fp, _, fsize in files:
            try:
                os.remove(fp)
                size -= fsize
                if size < MAX_EXP_SIZE:
                    logging.info("âœ… æ¸…ç†å®Œæ¯•ï¼Œç»éªŒæ± å·²æ¢å¤åˆ° 20GB ä»¥ä¸‹ã€‚")
                    break
            except Exception as e:
                logging.warning(f"æ— æ³•åˆ é™¤æ–‡ä»¶ {fp}: {e}")

def save_experience(image, action_dict):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    img_path = os.path.join(EXP_DIR, f"exp_{timestamp}.jpg")
    json_path = os.path.join(EXP_DIR, f"exp_{timestamp}.json")
    
    # ä¿å­˜å‹ç¼©å›¾ç‰‡ä»¥èŠ‚çœç©ºé—´
    image.save(img_path, format='JPEG', quality=80)
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(action_dict, f, ensure_ascii=False, indent=2)
    manage_experience_pool()

# ==========================================
# 7. æµè§ˆå™¨è·å–ä¸æ§åˆ¶
# ==========================================
def launch_and_get_browser():
    logging.info("æ£€æŸ¥å¹¶å¯åŠ¨ Firefox æµè§ˆå™¨...")
    firefox_windows = gw.getWindowsWithTitle('Mozilla Firefox')
    
    if not firefox_windows:
        if os.path.exists(FIREFOX_PATH):
            subprocess.Popen(FIREFOX_PATH)
            logging.info("å·²å‘é€å¯åŠ¨ Firefox çš„æŒ‡ä»¤ï¼Œç­‰å¾… 5 ç§’çª—å£å‡ºç°...")
            time.sleep(5)
            firefox_windows = gw.getWindowsWithTitle('Mozilla Firefox')
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æŒ‡å®šçš„æµè§ˆå™¨æ–‡ä»¶: {FIREFOX_PATH}")
    
    if not firefox_windows:
        raise RuntimeError("æ— æ³•æ‰¾åˆ° Firefox çª—å£ï¼Œè¯·ç¡®ä¿å®ƒå·²æ­£ç¡®å¯åŠ¨ä¸”æœªè¢«æ€æ¯’è½¯ä»¶æ‹¦æˆªã€‚")
    
    win = firefox_windows[0]
    if win.isMinimized:
        win.restore()
    win.activate()
    return win

# ==========================================
# 8. AI æ¨¡å‹åŠ è½½ (ä¸“ä¸º 4GB VRAM ä¼˜åŒ–)
# ==========================================
def load_ai_model():
    logging.info("æ­£åœ¨åŠ è½½ AI æ¨¡å‹ (Qwen2-VL-2B-Instruct)...")
    logging.info("â³ ä½ çš„ GPU æ˜¾å­˜ä¸º 4GBï¼Œæ­£åœ¨å¯ç”¨ 4-bit æé™å‹ç¼©é‡åŒ–...")
    
    os.environ['HF_HOME'] = MODEL_DIR
    
    try:
        model_id = "Qwen/Qwen2-VL-2B-Instruct"
        
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_id,
            device_map="auto",
            torch_dtype=torch.float16,
            load_in_4bit=True
        )
        processor = AutoProcessor.from_pretrained(model_id)
        logging.info("âœ… AI æ¨¡å‹åŠ è½½æˆåŠŸï¼æ˜¾å­˜å·²é©»ç•™ã€‚")
        return model, processor
    except Exception as e:
        logging.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
        logging.error(traceback.format_exc())
        raise e

# ==========================================
# 9. æ ¸å¿ƒ AI å†³ç­–é€»è¾‘
# ==========================================
def think_and_act(model, processor, browser_win):
    bbox = (browser_win.left, browser_win.top, browser_win.right, browser_win.bottom)
    if bbox[2] <= bbox[0] or bbox[3] <= bbox[1]:
        logging.warning("æµè§ˆå™¨çª—å£å°ºå¯¸å¼‚å¸¸æˆ–å·²æœ€å°åŒ–ï¼Œè·³è¿‡æ­¤æ¬¡æ“ä½œã€‚")
        time.sleep(1)
        return

    screenshot = ImageGrab.grab(bbox)
    
    # åŠ¨æ€ç¼©å°åˆ†è¾¨ç‡ï¼Œé˜²æ­¢æ¨ç†æ—¶æ˜¾å­˜æº¢å‡º (4GB æ˜¾å­˜æå…¶ç´§å¼ )
    max_dim = 800 
    screenshot.thumbnail((max_dim, max_dim))

    prompt_text = """ä½ æ˜¯ä¸€ä¸ªç”Ÿå­˜åœ¨æµè§ˆå™¨é‡Œçš„ AI æ™ºèƒ½ä½“ã€‚è§‚å¯Ÿä¸Šé¢çš„ç½‘é¡µæˆªå›¾ã€‚
è¯·æ€è€ƒä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆæ¥æ¢ç´¢ç½‘é¡µã€‚
ä½ å¿…é¡»ä¸”åªèƒ½è¾“å‡ºä¸€ä¸ª JSON å­—å…¸ã€‚
å¯ç”¨çš„ action:
1. "click": ç‚¹å‡»ã€‚æä¾› "x_ratio" å’Œ "y_ratio" (0.0~1.0 ç›¸å¯¹åæ ‡)ã€‚
2. "type": è¾“å…¥ã€‚æä¾› "text" å­—æ®µã€‚
3. "scroll": æ»šåŠ¨ã€‚æä¾› "amount" (è´Ÿæ•°å‘ä¸‹ï¼Œæ­£æ•°å‘ä¸Š)ã€‚
4. "wait": ç­‰å¾…è§‚å¯Ÿã€‚

ç¤ºä¾‹ï¼š
{"action": "click", "x_ratio": 0.5, "y_ratio": 0.2, "reason": "ç‚¹å‡»æœç´¢æ¡†"}
"""

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": screenshot},
                {"type": "text", "text": prompt_text},
            ],
        }
    ]

    try:
        # é¢„å¤„ç†
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to("cuda")

        # æ¨ç† (é™åˆ¶ max_new_tokens é™ä½æ˜¾å­˜å‹åŠ›)
        generated_ids = model.generate(**inputs, max_new_tokens=64)
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

    except Exception as e:
        logging.error(f"âŒ AI æ¨ç†æ—¶çˆ†æ˜¾å­˜æˆ–å‡ºé”™: {e}")
        logging.error(traceback.format_exc())
        return
    finally:
        # æåº¦æš´åŠ›çš„æ˜¾å­˜æ¸…ç†ï¼Œé˜²æ­¢ 4GB æ˜¾å¡ OOM å´©æºƒ
        if 'inputs' in locals(): del inputs
        if 'generated_ids' in locals(): del generated_ids
        torch.cuda.empty_cache()

    # è§£æ JSON
    try:
        if "```json" in output_text:
            output_text = output_text.split("```json")[1].split("```")[0].strip()
        elif "```" in output_text:
            output_text = output_text.split("```")[1].split("```")[0].strip()

        action_dict = json.loads(output_text)
        logging.info(f"ğŸ§  AI å†³å®šæ‰§è¡Œ: {action_dict}")
        
        execute_action(action_dict, browser_win)
        save_experience(screenshot, action_dict)
        
    except json.JSONDecodeError:
        logging.warning(f"âš ï¸ AI è¾“å‡ºäº†é JSON æ ¼å¼ï¼Œè·³è¿‡ã€‚åŸå§‹è¾“å‡º: {output_text}")
    except Exception as e:
        logging.error(f"âŒ æ‰§è¡ŒåŠ¨ä½œæ—¶å‡ºé”™: {e}")
        logging.error(traceback.format_exc())

def execute_action(action_dict, browser_win):
    action = action_dict.get("action")
    
    if action == "click":
        x_ratio = float(action_dict.get("x_ratio", 0.5))
        y_ratio = float(action_dict.get("y_ratio", 0.5))
        
        # ä¸¥æ ¼çš„åæ ‡è¶Šç•Œä¿æŠ¤
        target_x = browser_win.left + int(browser_win.width * x_ratio)
        target_y = browser_win.top + int(browser_win.height * y_ratio)
        target_x = max(browser_win.left + 5, min(target_x, browser_win.right - 5))
        target_y = max(browser_win.top + 5, min(target_y, browser_win.bottom - 5))
        
        pyautogui.moveTo(target_x, target_y, duration=0.4)
        pyautogui.click()
        
    elif action == "type":
        text = action_dict.get("text", "")
        # ä¸¥æ ¼è¿‡æ»¤ ESC é”®
        text_safe = text.replace("esc", "").replace("Escape", "")
        pyautogui.write(text_safe, interval=0.05)
        if "\n" in text:
            pyautogui.press('enter')
            
    elif action == "scroll":
        amount = int(action_dict.get("amount", -300))
        pyautogui.moveTo(
            browser_win.left + browser_win.width // 2,
            browser_win.top + browser_win.height // 2
        )
        pyautogui.scroll(amount)
        
    elif action == "wait":
        time.sleep(2)
        
    else:
        logging.warning(f"âš ï¸ é‡åˆ°æœªçŸ¥åŠ¨ä½œ: {action}")

# ==========================================
# 10. ä¸»ç¨‹åºç”Ÿå‘½å‘¨æœŸ
# ==========================================
def main():
    logging.info("="*55)
    logging.info(" ğŸ¤– æµè§ˆå™¨ AI æ™ºèƒ½ä½“ (4GB VRAM ä¼˜åŒ–ç‰ˆ) - å¯åŠ¨åºåˆ— ")
    logging.info(" ğŸ›¡ï¸  éšæ—¶é•¿æŒ‰ [ESC] é”®ç»ˆæ­¢ç¨‹åº ")
    logging.info("="*55)
    
    init_environment()
    browser_win = launch_and_get_browser()
    model, processor = load_ai_model()
    
    logging.info("ğŸš€ ç³»ç»Ÿå°±ç»ªï¼ŒAI å·²æ¥ç®¡æ§åˆ¶æƒ...")
    time.sleep(2)

    try:
        while True:
            # ç›‘å¬å…¨å±€é€€å‡ºæŒ‡ä»¤
            if keyboard.is_pressed('esc'):
                logging.info("ğŸ›‘ æ£€æµ‹åˆ°ç”¨æˆ·æŒ‰ä¸‹ [ESC] é”®ï¼Œæ­£åœ¨å®‰å…¨ç»ˆæ­¢ç¨‹åº...")
                break
                
            try:
                browser_win = gw.getWindowsWithTitle('Mozilla Firefox')[0]
                browser_win.activate()
            except IndexError:
                logging.error("âŒ æµè§ˆå™¨çª—å£è¢«å…³é—­ï¼AI å¤±å»å®¿ä¸»ç¯å¢ƒï¼Œå³å°†é€€å‡ºã€‚")
                break
                
            think_and_act(model, processor, browser_win)
            
            # ç»™ 3050Ti æ˜¾å¡ä¸€ç‚¹â€œå–˜æ¯â€çš„æ—¶é—´ï¼Œé˜²æ­¢è¿‡çƒ­å’Œé©±åŠ¨å‡æ­»
            time.sleep(2)
            
    except Exception as e:
        logging.error("âŒ ä¸»å¾ªç¯å‘ç”Ÿæœªé¢„æœŸçš„å´©æºƒï¼")
        logging.error(traceback.format_exc())
    finally:
        logging.info("ğŸ§¹ æ­£åœ¨é‡Šæ”¾æ˜¾å­˜å¹¶å…³é—­ç¨‹åº...")
        if 'model' in locals():
            del model
        torch.cuda.empty_cache()
        logging.info("âœ… ç¨‹åºå·²å®Œå…¨é€€å‡ºã€‚ç»éªŒæ± ä¿å­˜åœ¨ Desktop/AAA/Experience Poolã€‚")

if __name__ == "__main__":
    main()
